![Meta learning (Binz et al., 2024)](/assets/images/meta_learning.png)
From Binz, M., Dasgupta, I., Jagadish, A. K., Botvinick, M., Wang, J. X., & Schulz, E. (2023). *[Meta-learned models of cognition](https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/metalearned-models-of-cognition/F95059E07AE6E82AE56C4164A5384A18 "Binz et al., 2023")*. Behavioral and Brain Sciences, 1-38.

# About the workshop

In-context learning refers to the ability of a neural network to learn from information presented in its context. While traditional learning in neural networks requires adjusting network weights for every new task, in-context learning operates purely by updating internal activations without needing any updates to network weights. The emergence of this ability in large language models has led to a paradigm shift in machine learning and has forced researchers to reconceptualize how they think about learning in neural networks. Looking beyond language models, we can find in-context learning in many computational models relevant to cognitive science, including those that emerge from meta-learning. 

This workshop presented at [COGSCI 2024](https://cognitivesciencesociety.org/cogsci-2024/ "COGSCI 2024 Website") aims to delineate and discuss the implications of this phenomenon for the cognitive sciences. For this, we have invited a diverse group of researchers to map out the following questions:
- How well can human learning be modeled using in-context learning? 
- Which neural architectures support in-context learning?
- When and why do natural and artificial systems rely on in-context versus in-weights learning?
- How does in-context learning relate to classical concepts from cognitive science?


# Program

| **Time (CEST)** | **Speaker** | **Title and abstract** |
|:-----|:-----|:-----|
| 10h | Marcel Binz | Introduction to in-context learning <br> *Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam id blandit est, vitae sollicitudin ligula. Donec dictum sapien libero, quis iaculis turpis laoreet posuere. Praesent lacus mauris, varius a ante sed, egestas varius ligula. Proin iaculis rutrum faucibus. Morbi vitae lorem quis urna condimentum faucibus non bibendum metus. Aliquam eu malesuada justo. Sed vitae arcu eget urna egestas sagittis quis eget elit. Donec urna ante, tempor eu tellus eu, ultrices dictum libero. Suspendisse potenti.*|
| TBD | Stephanie Chan | What do you need for in-context learning in transformers? |
| TBD | Roma Patel | Towards understanding the (conceptual) structure of language models |
| TBD | Break ||
| TBD | Ishita Dasgupta | Concepts and categories within context |
| TBD | Akshay K. Jagadish | Ecologically rational meta-learned inference explains human category learning |
| TBD | Jacques Pesnot Lerousseau | Training data distribution drives in-context learning in humans and transformers |
| TBD | Break ||
| TBD | James Whittington | Different algorithms for in-context learning in prefrontal cortex and the hippocampal formation |
| TBD | Greta Tucktuke | Modeling human language processing using large language model |
| TBD | Tom McCoy | Understanding and controlling neural networks through the problem they are trained to solve |
| TBD | Break ||
| TBD | Christopher Summerfield <br> Morgan Barense <br> Alison Gopnick <br> Tom Griffiths <br> Micha Heilbron <br> Brenden Lake | Panel discussion |


# Speakers

### Stephanie Chan
Stephanie Chan is a senior research scientist at Google Deepmind. Having a background in both cognitive and computer science, she studies how data distributional properties drive emergent in-context learning.

### Roma Patel
Roma Patel is a fourth-year PhD student at Brown University. Her research uses language to structure reinforcement learning, aiming towards building more intelligent and interpretable agents that can learn to use language to communicate and coordinate with each other.

### James Whittington
James Whittington is a Sir Henry Wellcome postdoctoral fellow at Stanford University & the University of Oxford. He works on building models and theories for understanding structured neural representations in brains and machines.

### Greta Tucktuke
Greta Tucktuke is a PhD candidate at the Department of Brain and Cognitive Sciences at MIT. She studies how language is processed in the biological brain, and how the representations and processes in artificial neural networks models compare to those in humans. 

### Tom McCoy
Tom McCoy is an assistant professor in the Department of Linguistics at Yale University. He studies the computational principles that underlie human language using techniques from cognitive science, machine learning, and natural language processing.


# Organisers

### Marcel Binz
Marcel Binz is a research scientist at Helmholtz Munich. He works on modeling human behavior using ideas from meta-learning, resource rationality, and language models. 

### Akshay K. Jagadish 
Akshay K. Jagadish is a PhD student at the Max Planck Institute for Biological Cybernetics, T Ìˆubingen. His current research is dedicated towards understanding the ingredients essential for explaining human adaptive behavior across multiple task domains.

### Ishita Dasgupta
Ishita Dasgupta is a research scientist at Google Deepmind. She uses advances in machine learning to build models of human reasoning, applies cognitive science approaches toward understanding black-box AI systems, and combines these insights to build better, more human-like artificial intelligence.

### Jacques Pesnot Lerousseau
Jacques Pesnot Lerousseau is a postdoc at the Institute for Language, Communication, and the Brain, Marseille. His current research addresses the question of in-context learning in human brains and artificial neural networks, aiming to uncover the mechanisms behind rule generalization in the brain and algorithms. 


 

